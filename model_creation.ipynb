{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ff313e-3ae0-421f-b78c-1a6d04bd8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0024ac-799b-477a-a0a6-c504177de25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PracticeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ##First Layer\n",
    "        self.fc1 = nn.Linear(784,128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        ##Second Layer\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.relu2= nn.ReLU()\n",
    "        ## Final\n",
    "        self.fc3 = nn.Linear(64,10)\n",
    "    \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8274f7-10bf-410f-9e5d-0b8be663feb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PracticeModel(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = PracticeModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eefbd4d-fb04-4385-b806-22a5ff4155db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion)\n",
    "#optimiser\n",
    "optimiser = optim.Adam(model.parameters(),lr=0.01)\n",
    "print(optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7459fc6a-2179-43d6-9e26-070b2147b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Step\n",
    "def training_step(model,data,target,criterion,optimiser):\n",
    "    output = model(data)\n",
    "    loss = criterion(output,target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92e83537-70b8-4018-ace7-8759ca79816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage in a loop:\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42623927-1799-41fb-9dde-9570adb4d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, data):\n",
    "    # Set model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient computation for faster inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get predicted class (argmax)\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90774397-5fef-4ce4-b8d5-c1cf00f59b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filepath):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),      # Model weights\n",
    "        'optimizer_state_dict': optimizer.state_dict(),  # Optimizer state\n",
    "        'loss': loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved to {filepath}\")\n",
    "\n",
    "# Usage:\n",
    "# save_checkpoint(model, optimizer, epoch=5, loss=0.234, filepath='checkpoints/ckpt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b8e780-775d-40c6-a5de-ae5658370103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load optimizer state\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Extract metadata\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    \n",
    "    print(f\"Loaded checkpoint from epoch {epoch} with loss {loss}\")\n",
    "    return model, optimizer, epoch, loss\n",
    "\n",
    "# Usage:\n",
    "# model, optimizer, start_epoch, last_loss = load_checkpoint(model, optimizer, 'checkpoints/ckpt.pt')\n",
    "# for epoch in range(start_epoch + 1, num_epochs):\n",
    "#     # Resume training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff146123-809c-4d14-a1da-e234a33deb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_checkpointing(model, train_loader, val_loader, num_epochs, checkpoint_dir='checkpoints'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_data, batch_target in train_loader:\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_target in val_loader:\n",
    "                output = model(batch_data)\n",
    "                loss = criterion(output, batch_target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model, optimizer, epoch, val_loss, \n",
    "                          f'{checkpoint_dir}/best_model.pt')\n",
    "            print(f\"  → Saved best model!\")\n",
    "        \n",
    "        # Save regular checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            save_checkpoint(model, optimizer, epoch, train_loss,\n",
    "                          f'{checkpoint_dir}/ckpt_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0df39f4-5c6b-4e79-892b-a55d5c6cc0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create dummy data for demonstration\n",
    "X_train = torch.randn(1000, 784)  # 1000 samples, 784 features\n",
    "y_train = torch.randint(0, 10, (1000,))  # 10 classes\n",
    "X_val = torch.randn(200, 784)\n",
    "y_val = torch.randint(0, 10, (200,))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Create directories\n",
    "import os\n",
    "os.makedirs('checkpoints', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e93c9ad8-c4df-4581-9b5a-b47dac94b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a99a46b0-6511-4dbc-81b1-24e30d9ae286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss=2.3094, Val Loss=2.2971\n",
      "  ✓ Best model saved!\n",
      "Epoch 2/10: Train Loss=2.0458, Val Loss=2.3220\n",
      "Epoch 3/10: Train Loss=1.5172, Val Loss=2.4669\n",
      "Epoch 4/10: Train Loss=0.6526, Val Loss=2.8814\n",
      "Epoch 5/10: Train Loss=0.1482, Val Loss=3.2911\n",
      "Epoch 6/10: Train Loss=0.0427, Val Loss=3.5413\n",
      "Epoch 7/10: Train Loss=0.0212, Val Loss=3.6721\n",
      "Epoch 8/10: Train Loss=0.0138, Val Loss=3.7669\n",
      "Epoch 9/10: Train Loss=0.0100, Val Loss=3.8577\n",
      "Epoch 10/10: Train Loss=0.0076, Val Loss=3.9285\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_data, batch_target in train_loader:\n",
    "        output = model(batch_data)\n",
    "        loss = criterion(output, batch_target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_target in val_loader:\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_target)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "    \n",
    "    # Save best checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, 'checkpoints/best_model.pt')\n",
    "        print(f\"  ✓ Best model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdda48cf-9319-4855-b1b5-f40ff74d037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([5, 5, 5, 9, 6, 5, 7, 5, 5, 5])\n",
      "Confidence: tensor([0.1180, 0.1301, 0.1089, 0.1226, 0.1104, 0.1236, 0.1213, 0.1245, 0.1183,\n",
      "        0.1203])\n"
     ]
    }
   ],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint = torch.load('checkpoints/best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Perform inference\n",
    "model.eval()\n",
    "test_data = torch.randn(10, 784)  # 10 new samples\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_data)\n",
    "    predictions = torch.argmax(output, dim=1)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Confidence:\", probabilities.max(dim=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "516851b8-b07c-4621-8ea2-f96678a1c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint = torch.load('checkpoints/best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "# Continue training\n",
    "for epoch in range(start_epoch, num_epochs + 5):\n",
    "    # Training loop continues...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94a091-329d-4139-9954-2f6782b49a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
